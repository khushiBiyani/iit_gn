{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushiBiyani/iit_gn/blob/main/day1/logistic_regression_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with Gradient Descent algorithm"
      ],
      "metadata": {
        "id": "Xc7auBxUywqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "keN1QTCpsh2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2AahcFRsStK"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt \n",
        "from functools import partial\n",
        "#hihihii\n",
        "from sklearn.datasets import make_blobs, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate dataset"
      ],
      "metadata": {
        "id": "SCmT_caDstVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Blobs\n",
        "x, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=2, random_state=1)\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y);"
      ],
      "metadata": {
        "id": "eSKmcBZassp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform features\n",
        "\n",
        "Let us add bias using `PolynomialFeatures`."
      ],
      "metadata": {
        "id": "oRDtx_fB66gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oYuac1LY66gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "poly = PolynomialFeatures(degree=1, include_bias=True)\n",
        "x_transformed = poly.fit_transform(x)\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "5f2k6SLy66gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split\n",
        "\n",
        "* Split data into 80:20 train:test split.\n",
        "* Use 10% of the train data as validation data.\n",
        "\n",
        "> Hint: use `train_test_split` function from scikit-learn"
      ],
      "metadata": {
        "id": "CgWzAAz10Ub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vs0dUKvV5xzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "def get_train_val_test_split(x, y):\n",
        "  x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, train_size=0.8, random_state=0)\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.1, random_state=0)\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = get_train_val_test_split(x_transformed, y)\n",
        "x_train.shape, x_val.shape, x_test.shape"
      ],
      "metadata": {
        "id": "CVY1rHYe0xP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize parameters\n",
        "\n",
        "Initialize learnable parameters for the above dataset. You can use `jnp` just like regular numpy."
      ],
      "metadata": {
        "id": "f00VwAeJvefm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a5qYk_vb55J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "theta = jnp.array([0.1, 0.2, 0.3])"
      ],
      "metadata": {
        "id": "SeBrDWc6vq3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function\n",
        "\\begin{equation}\n",
        "\\operatorname{Loss}=-\\frac{1}{\\substack{\\text { output } \\\\ \\text { size }}} \\sum_{i=1}^{\\substack{\\text { output } \\\\ \\text { size }}} y_{i} \\cdot \\log \\hat{y}_{i}+\\left(1-y_{i}\\right) \\cdot \\log \\left(1-\\hat{y}_{i}\\right)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "JbpklCp9tt58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the cross entropy loss function which takes parameters as its first input.\n",
        "\n",
        "> Hint: You can use the following sigmoid function: `jax.nn.sigmoid`"
      ],
      "metadata": {
        "id": "pi0us_YBvLpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5hEEErTE6Bbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "\n",
        "def predict_fn(theta, x):\n",
        "  logits = jnp.dot(x, theta) # OR x@theta\n",
        "  y_pred = jax.nn.sigmoid(logits)\n",
        "  return y_pred\n",
        "\n",
        "def loss_fn(theta, x, y):\n",
        "  y_pred = predict_fn(theta, x)\n",
        "  loss = -(y * jnp.log(y_pred) + (1-y) *  jnp.log(1-y_pred))\n",
        "  return loss.mean()"
      ],
      "metadata": {
        "id": "OaaQ85qCtSLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the loss function at initial parameters"
      ],
      "metadata": {
        "id": "jZZbDvn2264J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eMFKZIBQ6Uzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "loss_fn(theta, x_train, y_train)"
      ],
      "metadata": {
        "id": "sibxmfpL2_t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the gradient function\n",
        "\n",
        "In JAX, you can apply `jax.grad` function on your loss function to get corresponding gradient function. The gradient function takes same arguments as the loss function but it returns the gradient of loss instead of loss.\n",
        "\n",
        "`jax.grad` takes your loss function as input and outputs the gradient function."
      ],
      "metadata": {
        "id": "ksErVEV1wefB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qWtcsS5j6WVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "grad_fn = jax.grad(loss_fn)"
      ],
      "metadata": {
        "id": "5tDasNDKwdZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define `learning_rate` and `n_iterations`\n",
        "\n",
        "One should choose appropriate `learning_rate` and `n_iterations` to make sure loss converges. You can set initial values based on your intuition and then adjust them based on loss convergence diagnostics."
      ],
      "metadata": {
        "id": "4_69f5pgxDNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WhfVAw2R6fzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "learning_rate = 0.1\n",
        "n_iterations = 100"
      ],
      "metadata": {
        "id": "sjFCc5qBxB9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize the parameters using the gradient descent algorithm\n",
        "\n",
        "* You should use the gradient function you have defined earlier.\n",
        "* You can implement a for loop running the algorithm `n_iterations` times.\n",
        "* You should append the training and validation loss in a list after each iteration for later diagnostics."
      ],
      "metadata": {
        "id": "60XPIFaJx-p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0rMY4v798ywv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "def train_fn(theta):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for iteration in range(n_iterations):\n",
        "    train_loss = loss_fn(theta, x_train, y_train)\n",
        "    val_loss = loss_fn(theta, x_val, y_val)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    grads = grad_fn(theta, x_train, y_train)\n",
        "    theta = theta - learning_rate * grads\n",
        "  return theta, train_losses, val_losses\n",
        "\n",
        "theta = jnp.array([0.1, 0.2, 0.3])\n",
        "theta, train_losses, val_losses = train_fn(theta)"
      ],
      "metadata": {
        "id": "UbU9U_Ayx9bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot loss v/s iterations\n",
        "\n",
        "This is one of the diagnostics we can use to check if our model has converged or not.\n",
        "\n",
        "* Plot training and validation loss v/s iterations"
      ],
      "metadata": {
        "id": "-mAk9nQR0Ekt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "def plot_train_val_loss(train_losses, val_losses):\n",
        "  plt.plot(train_losses, label='train')\n",
        "  plt.plot(val_losses, label='val')\n",
        "  plt.legend();\n",
        "\n",
        "plot_train_val_loss(train_losses, val_losses);"
      ],
      "metadata": {
        "id": "bFI1DxdT0C9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the decision boundary"
      ],
      "metadata": {
        "id": "hG0uRW3z4JZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WuQQN_399eJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "def plot_contour(show_proba=True):\n",
        "  x1 = jnp.linspace(x[:, 0].min()-0.5, x[:, 0].max()+0.5, 100)\n",
        "  x2 = jnp.linspace(x[:, 1].min()-0.5, x[:, 1].max()+0.5, 100)\n",
        "  X1, X2 = jnp.meshgrid(x1, x2)\n",
        "  X = jnp.array([(x11, x22) for x11, x22 in zip(X1.flatten(), X2.flatten())])\n",
        "  X = poly.transform(X)\n",
        "  y_pred = predict_fn(theta, X)\n",
        "  if not show_proba:\n",
        "    y_pred = y_pred >= 0.5\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y);\n",
        "  plt.contourf(X1, X2, y_pred.reshape(100, 100), alpha=0.5, cmap='coolwarm', levels=20);\n",
        "  plt.colorbar()\n",
        "\n",
        "plot_contour()"
      ],
      "metadata": {
        "id": "m4x2wvkW4DEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute accuracy on test dataset"
      ],
      "metadata": {
        "id": "dBtRfqdP8b47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bmq_Li-n9dNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "y_pred_proba = jax.nn.sigmoid(x_test@theta)\n",
        "y_pred = y_pred_proba >= 0.5\n",
        "accuracy_score(y_true=y_test, y_pred=y_pred)"
      ],
      "metadata": {
        "id": "1i9dmMn-8gQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try another dataset"
      ],
      "metadata": {
        "id": "uAqATPey8N9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Concentric circles\n",
        "x, y = make_circles(n_samples=100, random_state=0, noise=0.01)\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y);"
      ],
      "metadata": {
        "id": "_zDNNTDd7dlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform features\n",
        "\n",
        "Let us add bias using `PolynomialFeatures`."
      ],
      "metadata": {
        "id": "6qosp1EK9DwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TPiMkdy19DwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "poly = PolynomialFeatures(degree=1, include_bias=True)\n",
        "x_transformed = poly.fit_transform(x)\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "CTIqagEb9DwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test validation split as earlier"
      ],
      "metadata": {
        "id": "N6Xr9Xnz-Gzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GEr1t75u-LDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = get_train_val_test_split(x_transformed, y)\n",
        "x_train.shape, x_val.shape, x_test.shape"
      ],
      "metadata": {
        "id": "XZi1VUU3-Er1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize parameters using functions defined earlier"
      ],
      "metadata": {
        "id": "eh6tF4RN-ReZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "theta = jnp.array([0.1, 0.2, 0.3])\n",
        "n_iterations = 500\n",
        "theta, train_losses, val_losses = train_fn(theta)"
      ],
      "metadata": {
        "id": "WTfJyYAs8Qm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot loss v/s iterations"
      ],
      "metadata": {
        "id": "pOaGJt0K-YO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "qA42cvS4-BPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "plot_contour()"
      ],
      "metadata": {
        "id": "oOhy_2pTDSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "y_pred_proba = jax.nn.sigmoid(x_test@theta)\n",
        "y_pred = y_pred_proba >= 0.5\n",
        "accuracy_score(y_true=y_test, y_pred=y_pred)"
      ],
      "metadata": {
        "id": "M_vjncbUEDq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform features\n",
        "\n",
        "Current set of features are not able to learn the decision boundary properly. Let us include squared terms in our features. "
      ],
      "metadata": {
        "id": "bzuqOWAR_bdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aOWxdZ0v_c3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "x_transformed = poly.fit_transform(x)\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "LmlzZlIZ_d1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = get_train_val_test_split(x_transformed, y)\n",
        "x_train.shape, x_val.shape, x_test.shape"
      ],
      "metadata": {
        "id": "ce-nlqOHAYmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "theta = jnp.array([2.0, 1.0, 3.0, 1.0, 0.1, 0.5])\n",
        "n_iterations = 1000\n",
        "learning_rate= 0.1\n",
        "theta, train_losses, val_losses = train_fn(theta)"
      ],
      "metadata": {
        "id": "5AweKxY9DgbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(val_losses, label='val')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "wxD66SDoDnEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "\n",
        "plot_contour()"
      ],
      "metadata": {
        "id": "V3mNhHw1Dv3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution { display-mode: \"form\" }\n",
        "y_pred_proba = jax.nn.sigmoid(x_test@theta)\n",
        "y_pred = y_pred_proba >= 0.5\n",
        "accuracy_score(y_true=y_test, y_pred=y_pred)"
      ],
      "metadata": {
        "id": "e4YeFRnA9x1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta"
      ],
      "metadata": {
        "id": "u-G1Vc-a-T0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}